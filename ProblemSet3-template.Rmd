---
title: "Analysis on the popular vote of the 2020 American federal election"
author: "Yena Joo, Woolim Kim, Guemin Kim"
date: "Nov 2, 2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
library(tidyverse)
library(lme4)
library(knitr)
library(kableExtra)

# Loading in the cleaned survey Data
survey_data <- read_csv("survey_data.csv")

# exclude responses from those who are not eligible to vote
survey_data <- survey_data %>% filter(vote_intention != "No, I am not eligible to vote")

# Loading in the cleaned census Data
census_data <- read_csv("census_data.csv")

```

## Predictions on the 2020 US Presidential Election based on the voter survey responses.
*Code and data supporting this analysis is available at: https://github.com/Guemin/Problem_Set_3 *

# Model
As the 2020 presidential election of the United States approaches, people across the world are interested in to which candidate the vote of the US citizens will be concentrated, either to Donald Trump or to Joe Biden. Since the election outcome will also affect our community in Canada, we are going to analyze and predict the winner of the popular vote in the 2020 American federal election.  
Using the survey and census data obtained from Democracy Fund + UCLA Nationscape and IPUMS USA, we are going to predict the popular vote outcome of the election. To be more specific, we are going to use two logistic regression models, one for each candidate, and employ a post-stratification technique^[Post-stratification is a technique used in sample survey design to improve the quality of population estimates. In the post-stratification analysis, the population is partitioned into subgroups, and estimates are predicted within the subgroups. With the estimates, the sum of the estimate times the respective population size in each group is calculated, and finally, the sum is divided by the sum of the total population size. Detailed procedures on post-stratification for our analysis will be shown in the following sub-sections.] with the models.  
In the following sub-sections, we will describe the model specifics, the post-stratification calculation, and the result of the analysis.

## Model specifics

As already mentioned, we will be using the logistic regression models and post-stratification technique with R software to predict the proportions of voters who will vote for either Donald Trump or Joe Biden. Specifically, we will create two models, each for proportions of voters for Trump or Biden, using 6 different variables(age_group, gender, race, education, household_income , and state)^[* age_group is divided into 4 different groups: "18-29 year olds", "30-44 year olds", "45-64 year olds", "65 years and older".  
\    \    \ * gender indicates either "Male" or "Female".  
\    \    \ * race is divided into 5 different categories: "White", "Black", "Native", "Asian", "Other".  
\    \    \ * education is divided into 4 different categories: "Didn't graduate from high school", "High school graduate",   
\    \    \   "Some college or associate degree", "Bachelor's degree or higher".  
\    \    \ * household income consists of 9 categories range from "Less than $14,999" to "$150,000 and over".  
\    \    \ * state indicates abbreviated names of 52 states in the United States.].  

Since our response variables, vote_Trump and vote_Biden, are binary(either 'vote for' or 'not vote/not sure'), the logistic regression model is a suitable model to be used. Logistic regression is a mathematical model used to estimate the probability of an event occurring using binary data.  
The logistic regression models we are using are:

$$ log(\frac{p_{i}}{1-p_{i}}) = \beta_0+\beta_1  x_{age\ group} +\beta_2 x_{gender}+\beta_3 x_{race}+\beta_4x_{education}+\beta_5x_{household\ income}+\beta_6x_{state}$$

where $log(\frac{p_{i}}{1-p_{i}})$ represents log odds in each model, and $p_{i}$ is the proportions of voters who will vote for Donald trump or Joe Biden. Similarly, $\beta_0$ represents the intercept, and $\beta_1$,...,  $\beta_6$ indicate the slope parameters of the model. (Detailed descriptions on the x variables can be found in the footnote^[* $x_{age\ group}$ represents one of the four age groups that the respondent is in.  
\    \    \ * $x_{gender}$ indicates the gender of the respondent(either "Male" or "Female").  
\    \    \ * $x_{race}$ indicates the race ethnicity of the respondent.  
\    \    \ * $x_{education}$ indicates the education attainment of the respondent.  
\    \    \ * $x_{household\ income}$ indicates the total pre-tax income of the respondent's household.  
\    \    \ * $x_{state}$ indicates the state where the respondent is located in.]).  


Using the log odds estimates, we are going to find vote_Trump and vote_Biden (the proportions of voters each for Donald Trump and Joe Biden) in every possible combination of categories in our predictor variables, age_group, gender, race, education, household income, and state.

```{r, include=FALSE, warning= FALSE}
#remove na observations from the survey data
survey_data <- survey_data %>%
  filter(!is.na(age_group), !is.na(gender), !is.na(race), !is.na(education), !is.na(household_income), !is.na(state), !is.na(vote_Trump), !is.na(vote_Biden))
# create logistic regression models for each candidate

#vote for Trump
model_trump <- glm(vote_Trump ~ as.factor(age_group) + as.factor(gender) + as.factor(race) + as.factor(education) + as.factor(household_income) + as.factor(state), data=survey_data, family="binomial")

#voting for Biden
model_biden <- glm(vote_Biden ~ as.factor(age_group) + as.factor(gender) + as.factor(race) + as.factor(education) + as.factor(household_income) + as.factor(state), 
            data=survey_data, family="binomial")
```
**model diagnostics**  
Logistic regression could be well performed under several assumptions.  

The assumptions on logistic regression model are:  
1. linearity between the log odds and the predictor variables (independent variables should be linearly related to the log odds)  
2. independent errors (logistic regression requires each observation to be independent. )  
3. multicollinearity among predictors is not too high (predictor variables should be independent to each other)  

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Model diagnostics
par(mfrow = c(2, 2))
plot(residuals.glm(model_trump, type="pearson"),  ylab = "glm_residuals")
title("Figure 1")
plot(residuals.glm(model_biden, type="pearson"), ylab = "glm_residuals")
title("Figure 2")

# check for linear relationship between predictor variables and the log odds

# identify influential values
#par(mfrow = c(1, 2))
plot(model_trump, which =4)
title("Figure 3")
plot(model_biden, which =4)
title("Figure 4")

# check for multicollinearity of predictors in each model
vif_mod_t <- tibble(Predictor = c("age_group", "gender", "race", "education",
                                  "household_income", "state"), 
                    VIF = car::vif(model_trump)[,1])
vif_mod_b <- tibble(Predictor = c("age_group", "gender", "race", "education",
                                  "household_income", "state"), 
                    VIF = car::vif(model_biden)[,1])

kable(
  list(vif_mod_t, vif_mod_b),
  caption = 'VIF models',
  booktabs = TRUE, 
  valign = 't'
)

```

###Yena
1. Model diagnostics:  

2. Cook's distance is used in regression analysis to find influential outliers in predictor variables, which identifies points that negatively affect your regression model. Cook's distance is higher when residuals and leverage are high. (Infulential/outlier points can be removed for a better analysis)  

3. VIF is the Variance inflation factor, which measures the amount of multicollinearity in a set of multiple regression variables. Bigger the VIF, the bigger the multicollinearity is. As shown above, (Figure 3?) there is no sign of multicollinearity (not correlated) since VIF values do not exceed 2 for both models of Trump and Biden. Therefore, it is safe to say that the last assumption, multicollinearity among predictors is not too high is satisfied.  

###Model content
- Ages that are younger than 18 or older than 90(?) are removed from the data because...(also mention briefly what other samples are mutated/removed from the data)


## Post-Stratification 
- Any decisions that your group made should be explained and justified. (For example, are you looking at the proportion of people voting for Trump or Biden? why did you exclude sex in the cell split (practical explanations are acceptable)? etc.)  

In order to estimate the probabilities of voting for both Donald Trump and Joe Biden, we are going to perform a post-stratification analysis. In order to use this technique, we need to subdivide the population having similar characteristics into cells. Hence, we are going to create a total of 55,325 cells based on different age groups, gender, race-ethnicity, education attainment, household income, and state.  
Using the logistic regression models presented in the previous sub-section, we will estimate the probabilities of voting in each cell for each candidate. Then, we will weigh each estimate within each cell by the respective population size of the cell, sum those values, and divide that by the entire population size. This process can also be described by the expression:
$$\hat{y}^{ps}\ =\ \frac{\sum N_j*\hat{y_j}}{\sum{N_j}}$$
where $\hat{y_j}$ is the estimate of the probability of voting for either Trump or Biden in each cell, and $N_j$ is the population size of the $j^{th}$ cell based off demographics.

reason for Choice of the variables...

```{r include = FALSE, warning = FALSE, message=FALSE}
# Post-Stratification

###yps for glm Trump 
census_data$logodds_estimate_trump <-
  model_trump %>%
  predict(newdata = census_data)

census_data$estimate_trump <-
  exp(census_data$logodds_estimate_trump)/(1+exp(census_data$logodds_estimate_trump))

#overall prediction for Trump
predict_trump <- 
census_data %>%
  filter(!is.na(estimate_trump))%>%
  mutate(total_vote_prop_trump = estimate_trump*count) %>%
  summarise(total_predict_trump = sum(total_vote_prop_trump)/sum(count))

#using group_by(income)
predict_income_trump <- 
census_data %>%
  filter(!is.na(estimate_trump))%>%
  mutate(vote_prop_trump = estimate_trump*count) %>%
  group_by(household_income)%>%
  summarise(predict_trump= sum(vote_prop_trump)/sum(count))

#using group_by(state)
predict_state_trump <- 
census_data %>%
  filter(!is.na(estimate_trump))%>%
  mutate(vote_prop_trump2 = estimate_trump*count) %>%
  group_by(state)%>%
  summarise(predict_trump2= sum(vote_prop_trump2)/sum(count))

###yps for glm Biden
census_data$logodds_estimate_biden <-
  model_biden %>%
  predict(newdata = census_data)

census_data$estimate_biden <-
  exp(census_data$logodds_estimate_biden)/(1+exp(census_data$logodds_estimate_biden))

#overall prediction for Biden
predict_biden <- 
census_data %>%
  filter(!is.na(estimate_biden))%>%
  mutate(total_vote_prop_biden = estimate_biden*count) %>%
  summarise(total_predict_biden = sum(total_vote_prop_biden)/sum(count))

#using group_by(income)  
predict_income_biden <-
census_data %>%
  filter(!is.na(estimate_biden))%>%
  mutate(vote_prop_biden = estimate_biden*count) %>%
  group_by(household_income)%>%
  summarise(predict_biden = sum(vote_prop_biden)/sum(count))

#using group_by(state)
predict_state_biden <-
census_data %>%
  filter(!is.na(estimate_biden))%>%
  mutate(vote_prop_biden2 = estimate_biden*count) %>%
  group_by(state)%>%
  summarise(predict_biden2 = sum(vote_prop_biden2)/sum(count))

```



# Results

Here you will include all results. This includes descriptive statistics, graphs, figures, tables, and model results. Please ensure that everything is well formatted and in a report style. You must also provide an explanation of the results in this section. 

Please ensure that everything is well labelled. So if you have multiple histograms and plots, calling them Figure 1, 2, 3, etc. and referencing them as Figure 1, Figure 2, etc. in your report will be expected. The reader should not get lost in a sea of information. Make sure to have the results be clean, well formatted and digestible.

```{r echo=FALSE, message=FALSE, warning=FALSE}

#Model results
broom::tidy(model_trump)
broom::tidy(model_biden)
#knitr::kables(list(model_trump_tidy, model_biden_tidy))


#Post-Stratification result
## total probability of voting for Trump and Biden
kable(list(predict_trump, predict_biden), caption = "Comparison of predicted estimate between Trump and Biden")
#kables(list(
  #kable(head(predict_trump), "latex", align = "c"), 
  #kable(head(predict_biden), "latex", align = "c")))

#predict_trump
#predict_biden



```

We have created the logistic regression model on proportion of voters voting for Donald Trump and Joe Biden with 6 different following variables: age_group, gender, race, education, household_income, and state. Based off the post-stratification analysis we made, our estimation of the proportion of voters voting for Donald Trump is <0.433> and Joe Biden to be <0.394>.From the result of our estimation, We can predict that Donald Trump is more likely to win the 2020 president election.  

- individuals with household_income "less than $14,999" are more likely to vote for Biden over Trump (due to Biden's election promises for lower income people?)

- state: **idk**  
Using the estimate proportion grouped by states,  ......
```{r}
#compare the estimate for each state
trump_state <- predict_state_trump$predict_trump2
biden_state <- predict_state_biden$predict_biden2
trump_win <- 0 
biden_win <- 0
trump_biden <- c()
for (i in 1:51){
  if (trump_state[i] > biden_state[i]){
    trump_win <- trump_win + 1
    trump_biden <- c(trump_biden, "Trump")
  }
  else {biden_win <- biden_win + 1
  trump_biden <- c(trump_biden, "Biden")
}}
trump_win
biden_win
ggplot(data.frame(trump_biden), aes(x=trump_biden)) +
  geom_bar() + ggtitle("Figure n: Predicted Win Counts Per State") + xlab("Biden | Trump") + ylab("Count")
```
Under the assumption that whoever gets a higher expected proportion for each state wins in that state, Trump is expected to win in 31 states, and Biden is expected to win in 20 states. Both Figure x and Figure y show that Donald Trump has a higher possibility to win the election.   

# Discussion

Here you will summarize the previous sections and discuss conclusions drawn from the results. Make sure to elaborate and connect your analysis to the goal of the study.  

Using the survey and census data obtained from Democracy Fund + UCLA Nationscape and IPUMS USA, we have predicted the popular vote outcome of the 2020 president election in USA. Logistic Regression is used to predict who is more likely to be elected for the 2020 presidential election. Explanatory variables used for the logistic regression model are age_group, gender, race, education, household_income, and state. Then, $\hat{y}^{ps}$ is measured using post-stratification technique to estimate the proportion of voters in favor of voting for each candidate.  
- By using the post stratification, we created 55,325 cells based on the 6 variables that was used in the model, and found the probability of voting estimates for each cells.
- Then we grouped each cell estimates into states and predicted who is expected to have more 
- The result shows that estimate value for proportion of voters voting for Joe Biden is 39.4% and Donald Trump 43.34%. Also Trump is ahead of Biden by 21 counts in estimate for each state.
- *discuss about the result*  

To conclude, based off the estimated proportion of voters in favor of voting for Donald Trump being 0.4334 (43.34%) and expected to win 31 states, we predict that Trump will win the 2020 president election.  (......)

## Weaknesses

Here we discuss weaknesses of the study, data, analysis, etc. You can also discuss areas for improvement.
1. Weakness: Some variables could not be included in the generalized logistic model because either census data or survey data did not include the particular variables. If there is an important variable that could have affected the vote outcome, there might exist an omitted variable bias. (The omitted variables should be correlated with the dependent variable and with the explanatory variables included in the model).  

- the Census data used in the analysis is 2018 data, so it might not reflect the most accurate vote outcome. 2020 data is more suitable to analyze more accurate results. Also, people who were underage in 2016, hence not included in the estimate would have the right to vote in 2020.  


## Next Steps

Here you discuss subsequent work to be done after this report. This can include next steps in terms of statistical analysis (perhaps there is a more efficient algorithm available, or perhaps there is a caveat in the data that would allow for some new technique). Future steps should also be specified in terms of the study setting (eg. including a follow-up survey on something, or a subsequent study that would complement the conclusions of your report).  

- With the 2020 census data, we could estimate the proportion of voting for each candidate by state and estimate the winner of each state which would make a more reasonable and realistic prediction of the election.   
- Create visualization of the results to view the groups of the voting estimates at once.  
- In our future analysis, we can try to analyze the  multilevel regression models using Bayes coding techniques.  
- We can compare our prediction and the result of the actual 2020 president election.  
*(something about comparing with the actual election results and do a post-hoc analysis (or at least a survey) of how to better improve estimation in future elections.)*

# References
1. Survey data: https://www.voterstudygroup.org/downloads?key=9337162e-e5ef-49d7-96fd-48a5c5dba31c
2. Census data: https://usa.ipums.org/usa-action/extract_requests/summary?
3. Post-Stratification technique: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/04/forecasting-with-nonrepresentative-polls.pdf
4. Logit Regression Assumptions: https://rpubs.com/guptadeepak/logit-assumptions
5. Variance Inflation Factor(VIF): https://www.statisticshowto.com/variance-inflation-factor/
6. Tables side by side: https://bookdown.org/yihui/rmarkdown-cookbook/kable.html

#Appendix 
```{r echo=FALSE, message=FALSE, warning=FALSE}
## probabilites of voting for Trump and Biden in each state
#predict_state_trump
#predict_state_biden
knitr::kable(list(predict_state_trump, predict_state_biden))
#kables(list(
  #kable(predict_state_trump, "latex", align = "c"), 
  #kable(predict_state_biden, "latex", align = "c")), 
  #caption = "Figure n: ")

## probabilites of voting for Trump and Biden in each household income group
knitr::kable(list(predict_income_trump, predict_income_biden), caption = "Figure n ")
#predict_income_trump
#predict_income_biden
```


